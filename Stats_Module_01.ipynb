{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ques1) What is statistics, and why is it important?"
      ],
      "metadata": {
        "id": "o7C2by8jgQmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Statistics is the branch of mathematics that deals with collecting, organizing, analyzing, interpreting, and presenting data. It helps us make sense of large amounts of information by summarizing data into meaningful patterns or trends. Statistics is important because it provides tools to make informed decisions based on evidence rather than guesswork. Whether it's used in science to test hypotheses, in business to understand customer behavior, or in government to create policies, statistics plays a vital role in analyzing complex situations and predicting future outcomes. In essence, statistics allows us to draw conclusions and make decisions in the face of uncertainty."
      ],
      "metadata": {
        "id": "hXIPzPmugdg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques2) What are the two main types of statistics?"
      ],
      "metadata": {
        "id": "biJwYfVhgg01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The two main types of statistics are descriptive statistics and inferential statistics. Descriptive statistics involves methods for summarizing and organizing data so it can be easily understood. This includes measures such as averages, percentages, graphs, and charts that help describe the main features of a dataset. On the other hand, inferential statistics goes a step further by using data from a sample to make predictions or generalizations about a larger population. It involves techniques like hypothesis testing, confidence intervals, and regression analysis. Together, these two types of statistics provide a complete toolkit for analyzing data and drawing meaningful conclusions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dw2QjIGUgpF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques3) What are descriptive statistics?-"
      ],
      "metadata": {
        "id": "2Mo85aXxgp3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Descriptive statistics are methods used to summarize and organize data in a clear and understandable way. They provide simple summaries about the sample and the measures, helping to highlight the main features of a dataset without drawing conclusions beyond the data itself. Common tools in descriptive statistics include measures of central tendency like mean, median, and mode, as well as measures of variability such as range, variance, and standard deviation. Graphs, charts, and tables are also often used to visually represent the data. Descriptive statistics are essential for giving a quick overview of the data and are often the first step in any data analysis process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OAoyEA2Hgxe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques4) What is inferential statistics?"
      ],
      "metadata": {
        "id": "apfGfJnGg2Pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Inferential statistics is a branch of statistics that involves making predictions, decisions, or generalizations about a population based on data collected from a sample. Instead of simply describing the data, it allows researchers to draw conclusions and test hypotheses using various mathematical techniques. This process includes estimating population parameters, testing theories, and determining relationships between variables. Tools such as confidence intervals, significance tests, and regression analysis are commonly used in inferential statistics. It is particularly important because studying an entire population is often impractical or impossible, so inferential methods enable us to make educated assumptions with a certain level of confidence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c9TNTvnng6_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques5) What is sampling in statistics?"
      ],
      "metadata": {
        "id": "H51vQ80yg_x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sampling in statistics is the process of selecting a subset of individuals or observations from a larger population to represent the whole. Since it is often impractical or too costly to collect data from every member of a population, sampling allows researchers to gather and analyze data more efficiently. A well-chosen sample can provide reliable insights about the population, especially when appropriate sampling methods are used. There are various types of sampling, including random sampling, stratified sampling, and systematic sampling, each suited to different research needs. Proper sampling is crucial for ensuring that the results of a study are valid and can be generalized to the broader population.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gBwHCy9xhDOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques6) What are the different types of sampling methods?"
      ],
      "metadata": {
        "id": "ZgtPLtKdhIG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are several types of sampling methods in statistics, broadly categorized into probability and non-probability sampling. In probability sampling, every member of the population has a known and equal chance of being selected. Common types include simple random sampling, where individuals are chosen entirely by chance; stratified sampling, which divides the population into subgroups (strata) and samples from each; systematic sampling, where every nth member is selected; and cluster sampling, where entire groups or clusters are randomly chosen. In non-probability sampling, not every individual has a known or equal chance of being selected, and selection is often based on convenience or judgment. Examples include convenience sampling, judgmental or purposive sampling, and quota sampling. Choosing the right sampling method is essential for obtaining accurate and unbiased results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t_qRuAq-hMMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques7) What is the difference between random and non-random sampling?"
      ],
      "metadata": {
        "id": "zDkT1WQVhRLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The main difference between random and non-random sampling lies in how the samples are selected from the population. In random sampling, each member of the population has an equal and known chance of being selected, which helps ensure that the sample is representative and reduces the risk of bias. This makes the results more reliable and generalizable to the entire population. Examples include simple random sampling and stratified sampling. In contrast, non-random sampling does not give all members of the population an equal chance of selection; instead, samples are chosen based on factors like convenience or judgment. This can lead to bias and limits the ability to generalize the findings. While non-random sampling is often easier and quicker to conduct, it may not provide results that accurately reflect the broader population."
      ],
      "metadata": {
        "id": "OA0qTPFQhVRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques8) Define and give examples of qualitative and quantitative data?"
      ],
      "metadata": {
        "id": "2alIetelhaR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Qualitative data refers to non-numerical information that describes qualities or characteristics. It is often descriptive and can be categorized based on traits or labels but not measured in terms of numbers. Examples of qualitative data include colors of cars (red, blue, black), types of cuisine (Italian, Chinese, Indian), or customer feedback (satisfied, neutral, dissatisfied). On the other hand, quantitative data involves numerical values that can be counted or measured and used in mathematical calculations. It is typically divided into discrete data (whole numbers, like the number of students in a class) and continuous data (can take any value within a range, like height or temperature). Both types of data are crucial in statistical analysis, serving different purposes depending on the nature of the study.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-U2RvPc9hemz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques9) What are the different types of data in statistics?"
      ],
      "metadata": {
        "id": "S6eaz4njhkZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In statistics, data is generally classified into two main types: qualitative (categorical) data and quantitative (numerical) data, each with its own subtypes. Qualitative data describes attributes or categories and is divided into nominal data, which has no natural order (e.g., eye color, gender), and ordinal data, which has a meaningful order but unequal intervals (e.g., satisfaction levels like \"poor,\" \"average,\" \"good\"). Quantitative data, on the other hand, deals with numbers and is split into discrete data, which consists of countable values (e.g., number of siblings), and continuous data, which can take any value within a range (e.g., weight, temperature). Understanding these data types is essential for choosing the correct statistical methods and accurately interpreting results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fmEt_-5OhpQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques10) Explain nominal, ordinal, interval, and ratio levels of measurement?"
      ],
      "metadata": {
        "id": "poIUD9DnhugI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The four levels of measurement in statistics—nominal, ordinal, interval, and ratio—describe the nature of data and determine which statistical methods are appropriate to use. Nominal level is the most basic and involves data that can be categorized but not ranked or measured, such as blood types or hair colors. Ordinal level adds a sense of order or ranking to the categories, like education levels (e.g., high school, bachelor’s, master’s), but the intervals between the ranks are not necessarily equal. Interval level includes ordered data with equal intervals between values, such as temperature in Celsius or Fahrenheit, but lacks a true zero point, meaning ratios are not meaningful. Ratio level is the highest level, featuring all the properties of interval data plus a meaningful zero, allowing for the full range of arithmetic operations, including meaningful comparisons like \"twice as much.\" Examples include weight, height, and age. Understanding these levels is critical for selecting appropriate statistical techniques and accurately interpreting data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A7SNynTnhyVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques11) What is the measure of central tendency?"
      ],
      "metadata": {
        "id": "Bn--3Z9fh5rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The measure of central tendency refers to a statistical value that represents the center or typical value of a dataset. It provides a summary of the data by identifying a single value that best describes the distribution of the data. The three most commonly used measures of central tendency are the mean, median, and mode. The mean is the arithmetic average of all the values in a dataset, calculated by summing the values and dividing by the number of observations. The median is the middle value when the data is arranged in order, and it is especially useful when dealing with skewed distributions. The mode is the value that appears most frequently in the dataset. Each measure offers a different perspective on the data, and choosing the right one depends on the nature of the data and its distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "szGX13bGh93Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques12) Define mean, median, and mode?"
      ],
      "metadata": {
        "id": "ix1hgF9EiCsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The mean is the arithmetic average of a dataset, calculated by adding up all the values and then dividing by the number of values. It is useful for normally distributed data but can be influenced by extreme values, or outliers. The median, on the other hand, represents the middle value when the data is arranged in ascending or descending order. If there is an even number of values, the median is the average of the two middle numbers. The median is less affected by outliers and skewed data, making it a better measure for such datasets. The mode is the value that appears most frequently in a dataset. A dataset can have one mode (unimodal), more than one mode (bimodal or multimodal), or no mode at all if all values are unique. Each of these measures provides insight into the central tendency of the data, with the choice of measure depending on the distribution and characteristics of the dataset."
      ],
      "metadata": {
        "id": "kKRp6YXniGZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques13) What is the significance of the measure of central tendency?"
      ],
      "metadata": {
        "id": "_uUyDIFIiKyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The measure of central tendency is significant because it provides a single, representative value that summarizes a dataset, helping to describe the overall distribution of data. By offering a central or typical value, it simplifies complex data and makes it easier to understand and interpret. This is particularly important in areas like decision-making, research, and data analysis, where one needs to grasp the general trend or average behavior of a dataset. For instance, in business, understanding the mean sales figure can help predict future performance, while the median may be used to assess income levels where extreme values (outliers) could distort the interpretation. Additionally, the mode helps in identifying the most common occurrence, which can be useful in areas like marketing or product development. Ultimately, the measure of central tendency aids in comparing different datasets and providing a clearer picture of the data's distribution."
      ],
      "metadata": {
        "id": "q3mukPHsiO0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques14)  What is variance, and how is it calculated?"
      ],
      "metadata": {
        "id": "P0g4BNkEiTJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Variance is a statistical measure that represents the degree of spread or dispersion of a set of data points around the mean. It quantifies how much individual data points differ from the mean of the dataset. A higher variance indicates that the data points are more spread out, while a lower variance suggests that the data points are closer to the mean. To calculate variance, you first find the mean of the dataset, then subtract the mean from each data point to find the deviation of each value. These deviations are then squared to eliminate negative values, and the squared deviations are averaged.\n",
        "             \n"
      ],
      "metadata": {
        "id": "FoMzWu_UiWYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques15)  What is standard deviation, and why is it important?"
      ],
      "metadata": {
        "id": "QTMK7GEXjO4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Standard deviation is a measure of the amount of variation or dispersion in a set of data points. It is the square root of the variance and provides a more intuitive understanding of the spread of data, as it is expressed in the same units as the original data. A low standard deviation means the data points are close to the mean, indicating less variability, while a high standard deviation suggests the data points are spread out over a wider range, indicating more variability. Standard deviation is important because it allows for a better understanding of data consistency and is widely used in fields such as finance, engineering, and research. In particular, it helps in assessing the risk or uncertainty in a dataset, comparing different datasets, and making decisions based on how much variation is present around the average value.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ABfx5-NsjR48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques16) Define and explain the term range in statistics?"
      ],
      "metadata": {
        "id": "BDq-wd3pjYWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In statistics, the range is a simple measure of the spread or dispersion of a dataset. It is calculated by subtracting the smallest value (minimum) in the dataset from the largest value (maximum). The formula for range is:\n",
        "\n",
        "    Range =Maximum value − Minimum value\n",
        "\n",
        "  The range gives a quick sense of how spread out the values in a dataset are, but it has its limitations. Since it only considers the extremes (the maximum and minimum values), it is highly sensitive to outliers or extreme values that may not represent the overall distribution of the data. While it is easy to calculate and provides a basic understanding of the data's variability, the range alone might not give a complete picture, especially in datasets with outliers. Despite this, it is useful for getting an initial understanding of the data’s extent."
      ],
      "metadata": {
        "id": "9pHt87HrjiE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques17) What is the difference between variance and standard deviation?"
      ],
      "metadata": {
        "id": "586_jBBzjpfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The main difference between variance and standard deviation lies in their units and interpretation. Both are measures of the dispersion or spread of data points in a dataset, but variance is the average of the squared differences from the mean, while standard deviation is the square root of the variance. As a result, the variance is expressed in the squared units of the original data (for example, if the data is in meters, the variance will be in square meters), making it harder to interpret directly in the context of the data. On the other hand, the standard deviation is expressed in the same units as the data, which makes it more intuitive and easier to understand. In practice, standard deviation is more commonly used because it provides a clearer sense of how spread out the values are, while variance is often used in statistical calculations and theoretical analysis. Both are important, but the standard deviation is generally preferred for describing the variability of data in a more understandable way.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r9K0VtxBjxvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques18) What is skewness in a dataset?"
      ],
      "metadata": {
        "id": "sSXsySbYj3gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Skewness refers to the asymmetry or lack of symmetry in the distribution of data points in a dataset. It measures the extent to which the data is stretched or skewed to one side of the mean. A dataset is considered positively skewed (or right-skewed) when the right tail (larger values) is longer or fatter than the left tail, meaning the majority of the data points are clustered to the left of the mean. Conversely, a negatively skewed (or left-skewed) dataset has a longer or fatter left tail, with most data points concentrated to the right of the mean. Skewness is important because it can influence statistical analysis and the interpretation of the data. For instance, in a positively skewed dataset, the mean will typically be greater than the median, while in a negatively skewed dataset, the mean will usually be less than the median. Understanding skewness helps in selecting appropriate statistical methods and making more accurate predictions."
      ],
      "metadata": {
        "id": "ja5RRnhtj7TJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques19) What does it mean if a dataset is positively or negatively skewed?"
      ],
      "metadata": {
        "id": "-SvTiGKdj_KU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- When a dataset is positively skewed (right-skewed), it means that the right tail of the distribution is longer or more stretched out than the left tail. In other words, most of the data points are clustered towards the lower values, with a few larger values pulling the mean to the right. This results in a mean that is greater than the median, as the few larger values influence the average more than the middle value. For example, in income data, a few very high-income individuals can cause a positive skew, while the majority of people earn more typical, lower incomes.\n",
        "\n",
        "  On the other hand, a dataset that is negatively skewed (left-skewed) has a longer or more stretched out left tail. This indicates that most of the data points are concentrated towards the higher values, with a few very low values pulling the mean to the left. In this case, the mean will typically be less than the median, as the few low values exert more influence on the average. An example of negative skew might be age data in a retirement community, where most individuals are older, but a few younger people might pull the mean age down. Skewness in a dataset helps inform how the data is distributed and guides the selection of statistical techniques to analyze it accurately."
      ],
      "metadata": {
        "id": "cL4vWWhhkCb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques20) Define and explain kurtosis?"
      ],
      "metadata": {
        "id": "Am6--0M5kHtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Kurtosis is a statistical measure that describes the shape of a dataset's distribution, specifically its \"tailedness\" or the presence of outliers. It indicates how much data points are concentrated in the tails (extremes) or around the center of the distribution, compared to a normal distribution. There are three main types of kurtosis:\n",
        "\n",
        "  Leptokurtic: A distribution with high kurtosis, where the data has heavy tails or outliers, indicating a higher probability of extreme values. These distributions tend to have a sharper peak and more extreme values than a normal distribution.\n",
        "\n",
        "  Platykurtic: A distribution with low kurtosis, where the data has lighter tails and fewer extreme values. It appears flatter than a normal distribution, meaning there is less concentration of data at the extremes.\n",
        "\n",
        "  Mesokurtic: A distribution with kurtosis similar to that of a normal distribution, where the tails are neither particularly heavy nor light.\n",
        "\n",
        "  Kurtosis helps identify the potential for extreme events in a dataset, making it particularly useful in fields like finance and risk management, where understanding the likelihood of outliers or extreme outcomes is critical. A high kurtosis indicates that outliers are more likely, whereas low kurtosis suggests that extreme values are less likely."
      ],
      "metadata": {
        "id": "SCG3oC4ZkK8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques21) What is the purpose of covariance?"
      ],
      "metadata": {
        "id": "Vv8cyrKzkViJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Covariance is a measure of the relationship or association between two variables, showing how they change together. Specifically, it indicates whether an increase in one variable is associated with an increase or decrease in another. If the covariance is positive, it suggests that as one variable increases, the other also tends to increase (a positive relationship). If the covariance is negative, it indicates that as one variable increases, the other tends to decrease (a negative relationship). If the covariance is close to zero, it suggests little to no linear relationship between the variables.\n",
        "\n",
        "  The purpose of covariance is crucial in various fields such as finance, economics, and data analysis, as it helps to understand how two variables move in relation to each other. It is often used as a foundational concept for other statistical measures, like correlation, which standardizes covariance to provide a clearer understanding of the strength and direction of the relationship. However, covariance is affected by the scale of the variables, making it less intuitive to interpret compared to other measures like correlation."
      ],
      "metadata": {
        "id": "3CWGa4TtkbC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques22) What does correlation measure in statistics?-"
      ],
      "metadata": {
        "id": "G-x40Bplkgdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Correlation in statistics measures the strength and direction of the linear relationship between two variables. It quantifies how closely the changes in one variable are associated with changes in another. A positive correlation means that as one variable increases, the other also tends to increase, while a negative correlation indicates that as one variable increases, the other tends to decrease. The correlation coefficient, typically denoted as r, ranges from -1 to +1. A value of +1 indicates a perfect positive correlation, -1 represents a perfect negative correlation, and 0 means no linear relationship between the variables. Correlation is important because it helps to understand the degree to which two variables are related, and it can be used in predicting the behavior of one variable based on another. However, it is important to remember that correlation does not imply causation, meaning that just because two variables are correlated, it doesn’t necessarily mean that one causes the other to change.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fD7awSW7kkYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques23) What is the difference between covariance and correlation?"
      ],
      "metadata": {
        "id": "wldgVGb5koVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The primary difference between covariance and correlation lies in how they measure the relationship between two variables and the scale of their values. Covariance indicates the direction of the relationship between two variables (positive or negative) and shows whether the variables tend to increase or decrease together. However, covariance is influenced by the units of measurement of the variables, meaning it can be difficult to interpret directly, as its value depends on the scale of the data. For instance, covariance between height and weight might be much larger than the covariance between the number of hours studied and exam scores simply because the units (e.g., meters vs. number of hours) differ.\n",
        "\n",
        "  In contrast, correlation is a standardized version of covariance. It not only measures the direction of the relationship but also indicates the strength of the linear relationship between the two variables, with values ranging from -1 to +1. Correlation is unitless, meaning it is not affected by the scale of the variables, making it easier to interpret and compare across different datasets. A correlation of +1 or -1 indicates a perfect linear relationship, while 0 means no linear relationship. In summary, while both measures describe relationships between variables, correlation is more universally interpretable because it normalizes the relationship, making it independent of the data’s scale.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gjJDdNTckrcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques24)  What are some real-world applications of statistics?"
      ],
      "metadata": {
        "id": "We1a9S8lkwza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Statistics plays a crucial role in a wide range of real-world applications across various fields. In business, it is used to analyze customer behavior, optimize marketing strategies, and predict sales trends, enabling companies to make data-driven decisions. In healthcare, statistics helps in analyzing patient data, conducting clinical trials, and understanding the effectiveness of treatments, which are essential for improving medical outcomes. In economics, statistics is used to assess economic growth, inflation, and unemployment rates, guiding government policies and business investments. Sports teams use statistics to evaluate player performance, devise strategies, and improve training techniques. In education, statistical analysis helps in evaluating student performance, measuring the effectiveness of teaching methods, and improving curricula. Additionally, public policy decisions, such as determining resource allocation, are often based on statistical models that analyze demographic, social, and economic data. Overall, statistics is foundational to decision-making and problem-solving in nearly every aspect of modern life, helping to convert data into actionable insights.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QLTigHESk0Yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practical Questions"
      ],
      "metadata": {
        "id": "SKSu-54Zk39s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques1) How do you calculate the mean, median, and mode of a dataset?"
      ],
      "metadata": {
        "id": "xVXXRImlk8rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Example dataset\n",
        "data = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
        "\n",
        "# Calculating mean using NumPy\n",
        "mean = np.mean(data)\n",
        "\n",
        "# Calculating median using NumPy\n",
        "median = np.median(data)\n",
        "\n",
        "# Calculating mode using SciPy\n",
        "mode = stats.mode(data)[0][0]  # mode() returns an array, so we extract the first mode value\n",
        "\n",
        "# Output the results\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Median: {median}\")\n",
        "print(f\"Mode: {mode}\")\n"
      ],
      "metadata": {
        "id": "9FfsgewilCsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques2) Write a Python program to compute the variance and standard deviation of a dataset?"
      ],
      "metadata": {
        "id": "oV26RhomlIdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example dataset\n",
        "data = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
        "\n",
        "# Calculating variance using NumPy\n",
        "variance = np.var(data)\n",
        "\n",
        "# Calculating standard deviation using NumPy\n",
        "std_deviation = np.std(data)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Variance: {variance}\")\n",
        "print(f\"Standard Deviation: {std_deviation}\")\n"
      ],
      "metadata": {
        "id": "XZrNgHIAlMSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques3) Create a dataset and classify it into nominal, ordinal, interval, and ratio types?"
      ],
      "metadata": {
        "id": "_GTYB0WplTUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating a dataset\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Evan'],\n",
        "    'Education Level': ['Bachelor', 'Master', 'PhD', 'High School', 'Bachelor'],\n",
        "    'Age': [23, 30, 35, 19, 28],\n",
        "    'Temperature (°C)': [22, 25, 27, 20, 24],\n",
        "    'Height (cm)': [160, 175, 180, 165, 170]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Displaying the dataset\n",
        "print(\"Dataset:\")\n",
        "print(df)\n",
        "\n",
        "# Classifying the columns into types\n",
        "classification = {\n",
        "    'Nominal': ['Name'],\n",
        "    'Ordinal': ['Education Level'],\n",
        "    'Interval': ['Temperature (°C)'],\n",
        "    'Ratio': ['Height (cm)', 'Age']\n",
        "}\n",
        "\n",
        "# Output the classification\n",
        "print(\"\\nClassification into Nominal, Ordinal, Interval, and Ratio:\")\n",
        "for key, value in classification.items():\n",
        "    print(f\"{key}: {', '.join(value)}\")\n"
      ],
      "metadata": {
        "id": "2AJoPaTxlX5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques4) Implement sampling techniques like random sampling and stratified sampling?"
      ],
      "metadata": {
        "id": "REwDcyiXlizX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Creating a sample dataset\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Evan', 'Fay', 'George', 'Helen', 'Ivy', 'Jack'],\n",
        "    'Age': [23, 30, 35, 19, 28, 40, 55, 60, 33, 22],\n",
        "    'Gender': ['Female', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Female', 'Male']\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 1. Random Sampling:\n",
        "# We are taking a random sample of 4 rows from the dataset.\n",
        "random_sample = df.sample(n=4, random_state=42)\n",
        "\n",
        "# 2. Stratified Sampling:\n",
        "# We will stratify based on the 'Gender' column and take a random sample from each stratum\n",
        "stratified_sample = df.groupby('Gender', group_keys=False).apply(lambda x: x.sample(frac=0.5, random_state=42))\n",
        "\n",
        "# Output the results\n",
        "print(\"Original Dataset:\")\n",
        "print(df)\n",
        "\n",
        "print(\"\\nRandom Sample (4 rows):\")\n",
        "print(random_sample)\n",
        "\n",
        "print(\"\\nStratified Sample (50% from each gender group):\")\n",
        "print(stratified_sample)\n"
      ],
      "metadata": {
        "id": "k1vpmLTklnbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques5) Write a Python function to calculate the range of a dataset?"
      ],
      "metadata": {
        "id": "JBdbE1GUlvab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_range(data):\n",
        "    # Ensure the dataset is not empty\n",
        "    if len(data) == 0:\n",
        "        return None  # or raise an exception\n",
        "\n",
        "    # Calculate the range as the difference between the maximum and minimum values\n",
        "    data_range = max(data) - min(data)\n",
        "    return data_range\n",
        "\n",
        "# Example dataset\n",
        "data = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
        "\n",
        "# Calculate the range\n",
        "range_value = calculate_range(data)\n",
        "\n",
        "# Output the result\n",
        "print(f\"The range of the dataset is: {range_value}\")\n"
      ],
      "metadata": {
        "id": "g776iCGdlyxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques6) Create a dataset and plot its histogram to visualize skewness?"
      ],
      "metadata": {
        "id": "hXKrmnwPl3Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Create a sample dataset\n",
        "data = np.random.exponential(scale=2, size=1000)  # Exponentially distributed data (positively skewed)\n",
        "\n",
        "# Calculate skewness\n",
        "data_skewness = skew(data)\n",
        "\n",
        "# Plot the histogram\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data, kde=True, color='skyblue', bins=30)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(f'Histogram of the Dataset\\nSkewness: {data_skewness:.2f}', fontsize=15)\n",
        "plt.xlabel('Value', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Output the skewness value\n",
        "print(f\"Skewness of the dataset: {data_skewness:.2f}\")\n"
      ],
      "metadata": {
        "id": "xOPsDZdXl6zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques7) Calculate skewness and kurtosis of a dataset using Python libraries?"
      ],
      "metadata": {
        "id": "5n_sf0vxmAJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# Create a sample dataset (let's use a normal distribution for demonstration)\n",
        "data = np.random.normal(loc=0, scale=1, size=1000)  # Normally distributed data\n",
        "\n",
        "# Calculate skewness using scipy\n",
        "data_skewness = skew(data)\n",
        "\n",
        "# Calculate kurtosis using scipy\n",
        "data_kurtosis = kurtosis(data)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Skewness of the dataset: {data_skewness:.2f}\")\n",
        "print(f\"Kurtosis of the dataset: {data_kurtosis:.2f}\")\n"
      ],
      "metadata": {
        "id": "Z6Q9ib0vmFm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques8)  Generate a dataset and demonstrate positive and negative skewness?"
      ],
      "metadata": {
        "id": "ayKs_49vmK-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Generate a positively skewed dataset (exponential distribution)\n",
        "positive_skew_data = np.random.exponential(scale=2, size=1000)\n",
        "\n",
        "# Generate a negatively skewed dataset (inverse exponential distribution)\n",
        "negative_skew_data = 1 / np.random.exponential(scale=2, size=1000)\n",
        "\n",
        "# Calculate skewness for both datasets\n",
        "positive_skewness = skew(positive_skew_data)\n",
        "negative_skewness = skew(negative_skew_data)\n",
        "\n",
        "# Plotting both datasets and their histograms\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Positively skewed data plot\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(positive_skew_data, kde=True, color='skyblue', bins=30)\n",
        "plt.title(f\"Positively Skewed Data\\nSkewness: {positive_skewness:.2f}\", fontsize=14)\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Negatively skewed data plot\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(negative_skew_data, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(f\"Negatively Skewed Data\\nSkewness: {negative_skewness:.2f}\", fontsize=14)\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Output skewness values\n",
        "print(f\"Skewness of Positively Skewed Dataset: {positive_skewness:.2f}\")\n",
        "print(f\"Skewness of Negatively Skewed Dataset: {negative_skewness:.2f}\")\n"
      ],
      "metadata": {
        "id": "iq7ZdAWjmOrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques9) Write a Python script to calculate covariance between two datasets?"
      ],
      "metadata": {
        "id": "pY2GIynPmW0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example datasets\n",
        "dataset1 = np.array([10, 20, 30, 40, 50])\n",
        "dataset2 = np.array([5, 15, 25, 35, 45])\n",
        "\n",
        "# Calculate covariance using NumPy's cov function\n",
        "covariance_matrix = np.cov(dataset1, dataset2)\n",
        "\n",
        "# The covariance between the two datasets is the off-diagonal element of the covariance matrix\n",
        "covariance = covariance_matrix[0, 1]\n",
        "\n",
        "# Output the result\n",
        "print(f\"Covariance between the two datasets: {covariance}\")\n"
      ],
      "metadata": {
        "id": "i963JK8EmaWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques10)  Write a Python script to calculate the correlation coefficient between two datasets?"
      ],
      "metadata": {
        "id": "RB3lNmZ6mhIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example datasets\n",
        "dataset1 = np.array([10, 20, 30, 40, 50])\n",
        "dataset2 = np.array([5, 15, 25, 35, 45])\n",
        "\n",
        "# Calculate the correlation coefficient using NumPy's corrcoef function\n",
        "correlation_matrix = np.corrcoef(dataset1, dataset2)\n",
        "\n",
        "# The correlation coefficient is the off-diagonal element of the correlation matrix\n",
        "correlation_coefficient = correlation_matrix[0, 1]\n",
        "\n",
        "# Output the result\n",
        "print(f\"Correlation coefficient between the two datasets: {correlation_coefficient:.2f}\")\n"
      ],
      "metadata": {
        "id": "ojrxseqFmlXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques11) Create a scatter plot to visualize the relationship between two variables?"
      ],
      "metadata": {
        "id": "GuSZB9gbmqgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example datasets (two variables)\n",
        "dataset1 = np.array([10, 20, 30, 40, 50])\n",
        "dataset2 = np.array([5, 15, 25, 35, 45])\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(dataset1, dataset2, color='blue', edgecolor='black', s=100, alpha=0.7)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"Scatter Plot of Dataset1 vs Dataset2\", fontsize=15)\n",
        "plt.xlabel(\"Dataset 1 (X-axis)\", fontsize=12)\n",
        "plt.ylabel(\"Dataset 2 (Y-axis)\", fontsize=12)\n",
        "\n",
        "# Show grid for better visibility\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EAuV-z46mt6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques12)  Implement and compare simple random sampling and systematic sampling?"
      ],
      "metadata": {
        "id": "yNwdCmJsmzPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Creating a sample dataset (population)\n",
        "data = pd.DataFrame({\n",
        "    'ID': range(1, 101),  # IDs from 1 to 100\n",
        "    'Value': np.random.randint(1, 100, 100)  # Random values between 1 and 100\n",
        "})\n",
        "\n",
        "# Simple Random Sampling\n",
        "# Sample 10 items randomly from the dataset\n",
        "random_sample = data.sample(n=10, random_state=42)\n",
        "\n",
        "# Systematic Sampling\n",
        "# Select every 10th item from the dataset starting from a random position\n",
        "interval = 10\n",
        "start = np.random.randint(0, interval)  # Random starting point between 0 and 9\n",
        "systematic_sample = data.iloc[start::interval].head(10)  # Take every 10th element\n",
        "\n",
        "# Output the samples for comparison\n",
        "print(\"Simple Random Sampling:\")\n",
        "print(random_sample)\n",
        "\n",
        "print(\"\\nSystematic Sampling (every 10th element starting from random point):\")\n",
        "print(systematic_sample)\n"
      ],
      "metadata": {
        "id": "FhWX9zkkm4C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques13) Calculate the mean, median, and mode of grouped data?"
      ],
      "metadata": {
        "id": "6zDXMMfhm_Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define class intervals, frequencies and calculate midpoints\n",
        "class_intervals = [(0, 10), (10, 20), (20, 30), (30, 40), (40, 50)]\n",
        "frequencies = [5, 10, 15, 8, 2]\n",
        "\n",
        "# Calculate midpoints\n",
        "midpoints = [(interval[0] + interval[1]) / 2 for interval in class_intervals]\n",
        "\n",
        "# Mean calculation\n",
        "mean = np.sum(np.array(frequencies) * np.array(midpoints)) / np.sum(frequencies)\n",
        "\n",
        "# Cumulative frequencies\n",
        "cumulative_frequencies = np.cumsum(frequencies)\n",
        "\n",
        "# Median calculation\n",
        "N = np.sum(frequencies)  # Total frequency\n",
        "median_class_index = np.where(cumulative_frequencies >= N / 2)[0][0]  # Index of median class\n",
        "L = class_intervals[median_class_index][0]  # Lower boundary of the median class\n",
        "F = cumulative_frequencies[median_class_index - 1] if median_class_index > 0 else 0  # Cumulative frequency before median class\n",
        "f = frequencies[median_class_index]  # Frequency of the median class\n",
        "h = class_intervals[median_class_index][1] - class_intervals[median_class_index][0]  # Class width\n",
        "\n",
        "median = L + ((N / 2 - F) / f) * h\n",
        "\n",
        "# Mode calculation (finding the modal class)\n",
        "mode_class_index = np.argmax(frequencies)  # Index of the modal class\n",
        "L_mode = class_intervals[mode_class_index][0]  # Lower boundary of the modal class\n",
        "f1 = frequencies[mode_class_index]  # Frequency of the modal class\n",
        "f0 = frequencies[mode_class_index - 1] if mode_class_index > 0 else 0  # Frequency of the class before modal class\n",
        "f2 = frequencies[mode_class_index + 1] if mode_class_index < len(frequencies) - 1 else 0  # Frequency of the class after modal class\n",
        "\n",
        "mode = L_mode + ((f1 - f0) / (2 * f1 - f0 - f2)) * h\n",
        "\n",
        "# Output results\n",
        "print(f\"Mean: {mean:.2f}\")\n",
        "print(f\"Median: {median:.2f}\")\n",
        "print(f\"Mode: {mode:.2f}\")\n"
      ],
      "metadata": {
        "id": "U28IDqtHnC5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques14)Simulate data using Python and calculate its central tendency and dispersion?"
      ],
      "metadata": {
        "id": "ijHYvbTqnK6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Simulating a dataset of 1000 values from a normal distribution with mean=50 and std=10\n",
        "np.random.seed(42)  # For reproducibility\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)\n",
        "\n",
        "# Central Tendency Calculations:\n",
        "mean = np.mean(data)\n",
        "median = np.median(data)\n",
        "mode = stats.mode(data)[0][0]  # The mode() function returns an array, so we take the first element\n",
        "\n",
        "# Dispersion Calculations:\n",
        "variance = np.var(data)\n",
        "std_deviation = np.std(data)\n",
        "data_range = np.max(data) - np.min(data)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Mean: {mean:.2f}\")\n",
        "print(f\"Median: {median:.2f}\")\n",
        "print(f\"Mode: {mode:.2f}\")\n",
        "print(f\"Variance: {variance:.2f}\")\n",
        "print(f\"Standard Deviation: {std_deviation:.2f}\")\n",
        "print(f\"Range: {data_range:.2f}\")\n"
      ],
      "metadata": {
        "id": "OvbqOdeunQA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques15) Use NumPy or pandas to summarize a dataset’s descriptive statistics?"
      ],
      "metadata": {
        "id": "1mbXLCP9nWCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Simulating a dataset with 1000 data points\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)  # Normal distribution\n",
        "\n",
        "# Create a DataFrame for the data\n",
        "df = pd.DataFrame(data, columns=['Value'])\n",
        "\n",
        "# Summarize descriptive statistics using pandas\n",
        "summary = df.describe()\n",
        "\n",
        "# Output the summary\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "8Wkub-oQnbz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques16) Plot a boxplot to understand the spread and identify outliers?"
      ],
      "metadata": {
        "id": "NjACJQ1EngQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Simulating a dataset with 1000 data points\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)\n",
        "\n",
        "# Creating the boxplot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(data=data, color='skyblue')\n",
        "\n",
        "# Customizing the plot\n",
        "plt.title('Boxplot of Simulated Data', fontsize=15)\n",
        "plt.xlabel('Data Values', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eoPmFXl3pnzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques17) Calculate the interquartile range (IQR) of a dataset?"
      ],
      "metadata": {
        "id": "BpL3LTQcps0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulating a dataset\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)\n",
        "\n",
        "# Calculate the 1st and 3rd quartiles\n",
        "Q1 = np.percentile(data, 25)\n",
        "Q3 = np.percentile(data, 75)\n",
        "\n",
        "# Calculate the Interquartile Range (IQR)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Output the result\n",
        "print(f\"Interquartile Range (IQR): {IQR:.2f}\")\n"
      ],
      "metadata": {
        "id": "4fmEaZGzpxq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques18)  Implement Z-score normalization and explain its significance?"
      ],
      "metadata": {
        "id": "USdy3sODp2Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulating a dataset\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)\n",
        "\n",
        "# Calculate the mean and standard deviation\n",
        "mean = np.mean(data)\n",
        "std_dev = np.std(data)\n",
        "\n",
        "# Perform Z-score normalization\n",
        "z_scores = (data - mean) / std_dev\n",
        "\n",
        "# Output some of the Z-scores\n",
        "print(f\"First 10 Z-scores: {z_scores[:10]}\")\n",
        "\n",
        "# Plotting the normalized data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(z_scores, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "plt.title('Histogram of Z-scores', fontsize=15)\n",
        "plt.xlabel('Z-score', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bf2RTni7p6ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques19) Compare two datasets using their standard deviations?"
      ],
      "metadata": {
        "id": "jRn1YkFlqBSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulating two datasets\n",
        "np.random.seed(42)\n",
        "dataset_1 = np.random.normal(loc=50, scale=10, size=1000)  # Dataset with mean=50, std=10\n",
        "dataset_2 = np.random.normal(loc=50, scale=20, size=1000)  # Dataset with mean=50, std=20\n",
        "\n",
        "# Calculate the standard deviation for both datasets\n",
        "std_dev_1 = np.std(dataset_1)\n",
        "std_dev_2 = np.std(dataset_2)\n",
        "\n",
        "# Output the standard deviations\n",
        "print(f\"Standard Deviation of Dataset 1: {std_dev_1:.2f}\")\n",
        "print(f\"Standard Deviation of Dataset 2: {std_dev_2:.2f}\")\n",
        "\n",
        "# Compare the standard deviations\n",
        "if std_dev_1 > std_dev_2:\n",
        "    print(\"\\nDataset 1 has a higher spread (greater dispersion) than Dataset 2.\")\n",
        "elif std_dev_1 < std_dev_2:\n",
        "    print(\"\\nDataset 2 has a higher spread (greater dispersion) than Dataset 1.\")\n",
        "else:\n",
        "    print(\"\\nBoth datasets have the same spread.\")\n"
      ],
      "metadata": {
        "id": "g55CC4y4qExV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques20) Write a Python program to visualize covariance using a heatmap?"
      ],
      "metadata": {
        "id": "ntWlfE8TqK-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulating a dataset with 3 variables\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=(1000, 3))\n",
        "\n",
        "# Creating a pandas DataFrame for easier manipulation\n",
        "df = pd.DataFrame(data, columns=['Variable 1', 'Variable 2', 'Variable 3'])\n",
        "\n",
        "# Calculating the covariance matrix\n",
        "cov_matrix = df.cov()\n",
        "\n",
        "# Plotting the heatmap of the covariance matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cov_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, vmin=-30, vmax=30)\n",
        "\n",
        "# Customizing the plot\n",
        "plt.title('Covariance Matrix Heatmap', fontsize=15)\n",
        "plt.xlabel('Variables', fontsize=12)\n",
        "plt.ylabel('Variables', fontsize=12)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MjgaGelBqQi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques21) Use seaborn to create a correlation matrix for a dataset?"
      ],
      "metadata": {
        "id": "AdIc4mTGqU7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulating a dataset with 4 variables\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=(1000, 4))\n",
        "\n",
        "# Creating a pandas DataFrame\n",
        "df = pd.DataFrame(data, columns=['Variable 1', 'Variable 2', 'Variable 3', 'Variable 4'])\n",
        "\n",
        "# Calculating the correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Plotting the correlation matrix heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, vmin=-1, vmax=1)\n",
        "\n",
        "# Customizing the plot\n",
        "plt.title('Correlation Matrix Heatmap', fontsize=15)\n",
        "plt.xlabel('Variables', fontsize=12)\n",
        "plt.ylabel('Variables', fontsize=12)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UBU1-zPfqYY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques22) Generate a dataset and implement both variance and standard deviation computations?"
      ],
      "metadata": {
        "id": "_HtE1RHtqeAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generating a synthetic dataset\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)  # mean=50, std=10, n=1000\n",
        "\n",
        "# Manual computation of variance and standard deviation\n",
        "mean = np.mean(data)\n",
        "variance_manual = np.mean((data - mean) ** 2)\n",
        "std_dev_manual = np.sqrt(variance_manual)\n",
        "\n",
        "# Using NumPy built-in functions to compute variance and standard deviation\n",
        "variance_np = np.var(data)\n",
        "std_dev_np = np.std(data)\n",
        "\n",
        "# Outputting the results\n",
        "print(f\"Manual Computation of Variance: {variance_manual:.2f}\")\n",
        "print(f\"Manual Computation of Standard Deviation: {std_dev_manual:.2f}\")\n",
        "print(f\"NumPy Computed Variance: {variance_np:.2f}\")\n",
        "print(f\"NumPy Computed Standard Deviation: {std_dev_np:.2f}\")\n"
      ],
      "metadata": {
        "id": "9q0MV4ueqgz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques23) Visualize skewness and kurtosis using Python libraries like matplotlib or seaborn?"
      ],
      "metadata": {
        "id": "pXXJ0Zz0qlzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# Generate a dataset with positive skewness\n",
        "np.random.seed(42)\n",
        "positive_skew_data = np.random.chisquare(df=2, size=1000)  # Chi-square distribution\n",
        "\n",
        "# Generate a dataset with negative skewness\n",
        "negative_skew_data = np.random.beta(a=2, b=5, size=1000)  # Beta distribution\n",
        "\n",
        "# Generate a normal distribution (skew=0, kurtosis=3)\n",
        "normal_data = np.random.normal(loc=50, scale=10, size=1000)\n",
        "\n",
        "# Plotting skewness and kurtosis for different datasets\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Positive skew dataset\n",
        "sns.histplot(positive_skew_data, kde=True, color='blue', ax=axes[0])\n",
        "axes[0].set_title(f'Positive Skewness\\nSkew: {skew(positive_skew_data):.2f}, Kurtosis: {kurtosis(positive_skew_data):.2f}')\n",
        "\n",
        "# Negative skew dataset\n",
        "sns.histplot(negative_skew_data, kde=True, color='green', ax=axes[1])\n",
        "axes[1].set_title(f'Negative Skewness\\nSkew: {skew(negative_skew_data):.2f}, Kurtosis: {kurtosis(negative_skew_data):.2f}')\n",
        "\n",
        "# Normal distribution dataset (skew = 0, kurtosis = 3)\n",
        "sns.histplot(normal_data, kde=True, color='red', ax=axes[2])\n",
        "axes[2].set_title(f'Normal Distribution\\nSkew: {skew(normal_data):.2f}, Kurtosis: {kurtosis(normal_data):.2f}')\n",
        "\n",
        "# Show plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vrw-IWTwqo0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques24) Implement the Pearson and Spearman correlation coefficients for a dataset?"
      ],
      "metadata": {
        "id": "XmDzBYaHquqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "np.random.seed(42)\n",
        "x = np.random.normal(loc=50, scale=10, size=1000)  # Independent variable\n",
        "y = 2 * x + np.random.normal(loc=0, scale=5, size=1000)  # Dependent variable with some noise\n",
        "\n",
        "# Compute Pearson correlation coefficient\n",
        "pearson_corr, _ = pearsonr(x, y)\n",
        "\n",
        "# Compute Spearman correlation coefficient\n",
        "spearman_corr, _ = spearmanr(x, y)\n",
        "\n",
        "# Output the correlation results\n",
        "print(f\"Pearson Correlation Coefficient: {pearson_corr:.2f}\")\n",
        "print(f\"Spearman Correlation Coefficient: {spearman_corr:.2f}\")\n",
        "\n",
        "# Plotting the data and the relationship\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=x, y=y, color='blue', alpha=0.5)\n",
        "plt.title('Scatter Plot of x and y')\n",
        "plt.xlabel('x (Independent Variable)')\n",
        "plt.ylabel('y (Dependent Variable)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mn2_OOGiqxMk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}